{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":2960,"status":"ok","timestamp":1666471267520,"user":{"displayName":"Rafeed Rahman","userId":"00328865681086178046"},"user_tz":-480},"id":"pMTs8F1cvqvS"},"outputs":[],"source":["import json\n","import pandas as pd\n","import os\n","import sys\n","import pickle\n","import numpy as np\n","from random import random\n","import math\n","import torch\n","import torchvision \n","import torch.nn.functional as F  \n","import torchvision.datasets as datasets  \n","import torchvision.transforms as transforms  \n","from torch import optim  \n","from torch import nn  \n","from torch.utils.data import DataLoader  \n","from tqdm import tqdm  \n","from torch.utils.data import Dataset, DataLoader\n","from tqdm import tqdm"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1666471269334,"user":{"displayName":"Rafeed Rahman","userId":"00328865681086178046"},"user_tz":-480},"id":"9p2i1Iy92Wu1"},"outputs":[],"source":["parentPath = '/content/drive/MyDrive/PhD/Fragle_TSS/Tested_Algorithms/Large Bin Based Modeling/Dataset/Sig_10_Mb_10_Splits'\n","os.chdir(parentPath)"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1666471269335,"user":{"displayName":"Rafeed Rahman","userId":"00328865681086178046"},"user_tz":-480},"id":"HWcxDk0s2ZSE"},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model, criterion, optimizer = None, None, None\n","train_indices, test_indices = [], []\n","train_samples, test_samples, max_arr = None, None, None\n","train_meta_info, test_meta_info = None, None\n","\n","json_file = open('../../meta_info_files/split_patient_wise.json')\n","dic = json.load(json_file)\n","myPath = os.getcwd()\n","\n","thresholds = [0.05, 0.03, 0.02, 0.01, 0.005, 0.001]\n","MAE_dic, SN_dic, SP_dic = {}, {}, {}\n","for thr in thresholds:\n","    MAE_dic[thr] = []\n","    SN_dic[thr] = []\n","    SP_dic[thr] = []"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":16582,"status":"ok","timestamp":1666471288768,"user":{"displayName":"Rafeed Rahman","userId":"00328865681086178046"},"user_tz":-480},"id":"os4radFn4rm5"},"outputs":[],"source":["loaded_dict = {}\n","with open('train_samples.pkl', 'rb') as f:\n","  loaded_dict = pickle.load(f)\n","train_meta_info = loaded_dict['meta'] \n","train_samples = loaded_dict['samples']\n","\n","loaded_dict = {}\n","with open('test_samples.pkl', 'rb') as f:\n","  loaded_dict = pickle.load(f)\n","test_meta_info = loaded_dict['meta'] \n","test_samples = loaded_dict['samples']\n","loaded_dict = {}"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":729,"status":"ok","timestamp":1666471291576,"user":{"displayName":"Rafeed Rahman","userId":"00328865681086178046"},"user_tz":-480},"id":"hDyQ6OWi6Mvt"},"outputs":[],"source":["sums = np.sum(train_samples, axis=2)\n","train_samples = train_samples/ sums[:, :, np.newaxis]\n","\n","sums = np.sum(test_samples, axis=2)\n","test_samples = test_samples/ sums[:, :, np.newaxis]"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":30,"status":"ok","timestamp":1666471291577,"user":{"displayName":"Rafeed Rahman","userId":"00328865681086178046"},"user_tz":-480},"id":"z97LcNUR2g6c"},"outputs":[],"source":["class LoadDataset(Dataset):\n","    def __init__(self, indices, train):\n","        self.indices = indices\n","        self.is_train = train\n","\n","    def __len__(self):\n","        return len(self.indices)\n","\n","    def __getitem__(self, index):\n","        ind = self.indices[index]\n","        if self.is_train == True:\n","          dataY = torch.tensor(train_meta_info[ind][-1])\n","          dataX = torch.tensor(train_samples[ind]/ max_arr)\n","        else:\n","          dataY = torch.tensor(test_meta_info[ind][-1])\n","          dataX = torch.tensor(test_samples[ind]/ max_arr)\n","        dataX = dataX.float()\n","        dataY = dataY.float()\n","        return (dataX, dataY)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O8TnRJFu2mIt"},"outputs":[],"source":["class VGG_convnet(nn.Module):\n","\n","    def __init__(self, bin_no = 282, feature_no = 156):\n","\n","        super(VGG_convnet, self).__init__()\n","\n","        self.NN = nn.Sequential(nn.Linear(feature_no, 64), nn.ReLU(),\n","                            nn.Linear(64, 96), nn.ReLU(),\n","                            nn.Linear(96, 128), nn.ReLU())\n","        \n","        self.pos_emb1D = torch.nn.Parameter(torch.randn(bin_no, 128))\n","        \n","        # block 1:         \n","        self.conv1a = nn.Conv1d(in_channels=128, out_channels=32, kernel_size=3, padding=1)\n","        self.conv1b = nn.Conv1d(in_channels=32, out_channels=32, kernel_size=3, padding=1)\n","        self.LN1 = nn.LayerNorm(bin_no)\n","        \n","        # block 2:      \n","        self.conv2a = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n","        self.conv2b = nn.Conv1d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n","        self.LN2 = nn.LayerNorm(bin_no)\n","        \n","        # block 3:            \n","        self.conv3a =  nn.Conv1d(in_channels=64, out_channels=96, kernel_size=3, padding=1)\n","        self.conv3b =  nn.Conv1d(in_channels=96, out_channels=96, kernel_size=3, padding=1)\n","        self.LN3 = nn.LayerNorm(bin_no)\n","        \n","        #block 4:       \n","        self.conv4a = nn.Conv1d(in_channels=96, out_channels=128, kernel_size=3, padding=1)\n","        self.conv4b = nn.Conv1d(in_channels=128, out_channels=128, kernel_size=3, padding=1)\n","        self.LN4 = nn.LayerNorm(bin_no)\n","        \n","        # linear layers:\n","        self.Dense = nn.Sequential(nn.Linear(128, 128), nn.ReLU(),\n","                                   nn.Linear(128, 64), nn.ReLU(),\n","                                   nn.Linear(64, 1)) \n","\n","    def forward(self, x):\n","        x = self.NN(x)\n","        x = x + self.pos_emb1D\n","        x = x.permute(0, 2, 1)\n","        \n","        # block 1:  \n","        x = F.relu(self.conv1a(x))\n","        x = F.relu(self.conv1b(x))\n","        x = self.LN1(x)\n","        \n","        # block 2:   \n","        x = F.relu(self.conv2a(x))\n","        x = F.relu(self.conv2b(x))\n","        x = self.LN2(x)\n","\n","        # block 3:    \n","        x = F.relu(self.conv3a(x))\n","        x = F.relu(self.conv3b(x))\n","        x = self.LN3(x)\n","        \n","        #block 4:      \n","        x = F.relu(self.conv4a(x))\n","        x = F.relu(self.conv4b(x))\n","        x = self.LN4(x)\n","        \n","        y = torch.mean(x, axis=2)\n","        \n","        # linear layers:   \n","        y = self.Dense(y)\n","        y = y.squeeze()\n","        \n","        return y"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":491,"status":"ok","timestamp":1666471313950,"user":{"displayName":"Rafeed Rahman","userId":"00328865681086178046"},"user_tz":-480},"id":"TJywa6um3rtu"},"outputs":[],"source":["def custom_loss(scores, targets):\n","    const = 0.003\n","    numerator = const + torch.abs(scores - targets)\n","    denominator = const + targets\n","    total_loss = torch.sum(numerator/denominator)\n","    avg_loss = total_loss/ scores.size(0)\n","    return avg_loss"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1666471315359,"user":{"displayName":"Rafeed Rahman","userId":"00328865681086178046"},"user_tz":-480},"id":"jgcHv2hkEZAe"},"outputs":[],"source":["def metric_calc(loader, thr):\n","    running_MAE = 0\n","    num_batches = 0\n","    TP, TN, FP, FN = 0, 0, 0, 0\n","    model.eval()\n","    with torch.no_grad():\n","        for batch_idx, (data, targets) in enumerate(loader):\n","            # Get data to cuda if possible\n","            data = data.to(device=device)\n","            targets = targets.to(device=device)\n","\n","            # forward\n","            scores = model(data)\n","            scores = scores.view(targets.size())\n","            \n","            # metric calc (MAE)\n","            #MAE_batch = L1criterion(scores, targets)\n","            MAE_batch = criterion(scores, targets)\n","            running_MAE = running_MAE + MAE_batch.item()\n","            \n","            # TP, TN, FP, FN\n","            for i in range(targets.size(0)):\n","                # if targets[i].item()>0.0 and targets[i].item()<0.01:\n","                if targets[i].item()>0.0 and targets[i].item()<thr:\n","                     continue\n","                elif scores[i].item()>thr and targets[i].item()>0.0:\n","                    TP += 1\n","                elif scores[i].item()<=thr and targets[i].item()>0.0:\n","                    FN+=1\n","                elif scores[i].item()>thr and targets[i].item()==0.0:\n","                    FP+=1\n","                elif scores[i].item()<=thr and targets[i].item()==0.0:\n","                    TN+=1\n","\n","            num_batches+=1\n","    \n","    MAE_final = running_MAE/num_batches\n","    SN_final = TP/(TP+FN)\n","    SP_final = TN/(TN+FP)\n","    \n","    return MAE_final, SN_final, SP_final"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1666471322874,"user":{"displayName":"Rafeed Rahman","userId":"00328865681086178046"},"user_tz":-480},"id":"yeYPjUBl30X8"},"outputs":[],"source":["def train(train_loader):\n","  for epoch in range(70):\n","    for batch_idx, (data, targets) in enumerate(train_loader):\n","        # Get data to cuda if possible\n","        data = data.to(device=device)\n","        data.requires_grad_()\n","        targets = targets.to(device=device)\n","\n","        # forward\n","        scores = model(data)\n","        scores = scores.view(targets.size())\n","        # loss = criterion(scores, targets)\n","        loss = custom_loss(scores, targets)\n","        #loss = 2*loss1 + loss2\n","\n","        # backward\n","        optimizer.zero_grad()\n","        loss.backward()\n","\n","        # gradient descent or adam step\n","        optimizer.step()"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1666471326032,"user":{"displayName":"Rafeed Rahman","userId":"00328865681086178046"},"user_tz":-480},"id":"78h6fmNo39iT"},"outputs":[],"source":["def make_csv(split_no, max_arr):\n","    csv_list = []\n","    test_y = []\n","    scores, sample_names = [], []\n","    \n","    model.eval()\n","    with torch.no_grad():\n","      for index in test_indices:\n","          sample_names.append(test_meta_info[index][1])\n","          dataX = np.copy(test_samples[index])\n","          dataX = dataX/ max_arr\n","          test_y.append(test_meta_info[index][-1])\n","\n","          dataX = torch.tensor(dataX)\n","          # dataX = dataX.permute(1, 0)     ########\n","          dataX = torch.unsqueeze(dataX, dim=0)\n","          dataX = dataX.float()\n","\n","          score = model(dataX.to(device))\n","          scores.append(score.item())\n","        \n","    for i in range(len(scores)):\n","        tmp = []\n","        tmp.append(sample_names[i])\n","        tmp.append(scores[i])\n","        tmp.append(test_y[i])\n","        csv_list.append(tmp)\n","        \n","    folder = '/content/drive/MyDrive/PhD/Fragle_TSS/Tested_Algorithms/Large Bin Based Modeling/10_split_csv/'\n","    filePath = folder + 'test' + str(split_no) + '.csv'\n","    my_df = pd.DataFrame(csv_list)\n","    my_df.to_csv(filePath, index=False, header=['Sample_ID', 'Pred_Fraction', 'True_Fraction'])"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9uqjfrrZ4n_D","executionInfo":{"status":"ok","timestamp":1666472135515,"user_tz":-480,"elapsed":801037,"user":{"displayName":"Rafeed Rahman","userId":"00328865681086178046"}},"outputId":"840f730f-6476-476a-de8b-f36980a75218"},"outputs":[{"output_type":"stream","name":"stdout","text":["Split no. 0\n","threshold: 5.0%, MAE: 0.04374711621891369, Sensitivity: 0.8557046979865772, Specificity: 1.0\n","threshold: 3.0%, MAE: 0.04374711621891369, Sensitivity: 0.868421052631579, Specificity: 1.0\n","threshold: 2.0%, MAE: 0.04374711621891369, Sensitivity: 0.868663594470046, Specificity: 0.9629629629629629\n","threshold: 1.0%, MAE: 0.04374711621891369, Sensitivity: 0.8831967213114754, Specificity: 0.9629629629629629\n","threshold: 0.5%, MAE: 0.04374711621891369, Sensitivity: 0.8737864077669902, Specificity: 0.9259259259259259\n","threshold: 0.1%, MAE: 0.04374711621891369, Sensitivity: 0.8312829525483304, Specificity: 0.8518518518518519\n","\n","\n","Split no. 1\n","threshold: 5.0%, MAE: 0.041983038102361286, Sensitivity: 0.7781456953642384, Specificity: 1.0\n","threshold: 3.0%, MAE: 0.041983038102361286, Sensitivity: 0.7596899224806202, Specificity: 1.0\n","threshold: 2.0%, MAE: 0.041983038102361286, Sensitivity: 0.7945823927765236, Specificity: 0.9629629629629629\n","threshold: 1.0%, MAE: 0.041983038102361286, Sensitivity: 0.8256513026052105, Specificity: 0.9629629629629629\n","threshold: 0.5%, MAE: 0.041983038102361286, Sensitivity: 0.8633776091081594, Specificity: 0.9629629629629629\n","threshold: 0.1%, MAE: 0.041983038102361286, Sensitivity: 0.8576329331046312, Specificity: 0.7777777777777778\n","\n","\n","Split no. 2\n","threshold: 5.0%, MAE: 0.04028476947652442, Sensitivity: 0.8362989323843416, Specificity: 1.0\n","threshold: 3.0%, MAE: 0.04028476947652442, Sensitivity: 0.8705234159779615, Specificity: 0.9629629629629629\n","threshold: 2.0%, MAE: 0.04028476947652442, Sensitivity: 0.8992805755395683, Specificity: 0.9629629629629629\n","threshold: 1.0%, MAE: 0.04028476947652442, Sensitivity: 0.9150743099787686, Specificity: 0.8888888888888888\n","threshold: 0.5%, MAE: 0.04028476947652442, Sensitivity: 0.927710843373494, Specificity: 0.8148148148148148\n","threshold: 0.1%, MAE: 0.04028476947652442, Sensitivity: 0.9094202898550725, Specificity: 0.6666666666666666\n","\n","\n","Split no. 3\n","threshold: 5.0%, MAE: 0.04245148704979907, Sensitivity: 0.8762541806020067, Specificity: 1.0\n","threshold: 3.0%, MAE: 0.04245148704979907, Sensitivity: 0.8808290155440415, Specificity: 1.0\n","threshold: 2.0%, MAE: 0.04245148704979907, Sensitivity: 0.8981900452488688, Specificity: 0.9629629629629629\n","threshold: 1.0%, MAE: 0.04245148704979907, Sensitivity: 0.9357429718875502, Specificity: 0.7037037037037037\n","threshold: 0.5%, MAE: 0.04245148704979907, Sensitivity: 0.9619771863117871, Specificity: 0.48148148148148145\n","threshold: 0.1%, MAE: 0.04245148704979907, Sensitivity: 0.9621993127147767, Specificity: 0.4074074074074074\n","\n","\n","Split no. 4\n","threshold: 5.0%, MAE: 0.037485419857231056, Sensitivity: 0.8903654485049833, Specificity: 1.0\n","threshold: 3.0%, MAE: 0.037485419857231056, Sensitivity: 0.8808290155440415, Specificity: 1.0\n","threshold: 2.0%, MAE: 0.037485419857231056, Sensitivity: 0.9027149321266968, Specificity: 0.9629629629629629\n","threshold: 1.0%, MAE: 0.037485419857231056, Sensitivity: 0.9196787148594378, Specificity: 0.9629629629629629\n","threshold: 0.5%, MAE: 0.037485419857231056, Sensitivity: 0.9239543726235742, Specificity: 0.8518518518518519\n","threshold: 0.1%, MAE: 0.037485419857231056, Sensitivity: 0.9467353951890034, Specificity: 0.5185185185185185\n","\n","\n","Split no. 5\n","threshold: 5.0%, MAE: 0.034997709434140815, Sensitivity: 0.8361204013377926, Specificity: 1.0\n","threshold: 3.0%, MAE: 0.034997709434140815, Sensitivity: 0.828125, Specificity: 1.0\n","threshold: 2.0%, MAE: 0.034997709434140815, Sensitivity: 0.8613636363636363, Specificity: 0.9629629629629629\n","threshold: 1.0%, MAE: 0.034997709434140815, Sensitivity: 0.8810483870967742, Specificity: 0.7777777777777778\n","threshold: 0.5%, MAE: 0.034997709434140815, Sensitivity: 0.8797709923664122, Specificity: 0.7037037037037037\n","threshold: 0.1%, MAE: 0.034997709434140815, Sensitivity: 0.906896551724138, Specificity: 0.48148148148148145\n","\n","\n","Split no. 6\n","threshold: 5.0%, MAE: 0.04371624969115311, Sensitivity: 0.7594501718213058, Specificity: 1.0\n","threshold: 3.0%, MAE: 0.04371624969115311, Sensitivity: 0.8090185676392573, Specificity: 1.0\n","threshold: 2.0%, MAE: 0.04371624969115311, Sensitivity: 0.8614318706697459, Specificity: 1.0\n","threshold: 1.0%, MAE: 0.04371624969115311, Sensitivity: 0.918200408997955, Specificity: 0.8888888888888888\n","threshold: 0.5%, MAE: 0.04371624969115311, Sensitivity: 0.9264990328820116, Specificity: 0.7777777777777778\n","threshold: 0.1%, MAE: 0.04371624969115311, Sensitivity: 0.9284467713787086, Specificity: 0.48148148148148145\n","\n","\n","Split no. 7\n","threshold: 5.0%, MAE: 0.03660496684129943, Sensitivity: 0.8020477815699659, Specificity: 1.0\n","threshold: 3.0%, MAE: 0.03660496684129943, Sensitivity: 0.8121693121693122, Specificity: 0.9629629629629629\n","threshold: 2.0%, MAE: 0.03660496684129943, Sensitivity: 0.8410138248847926, Specificity: 0.9629629629629629\n","threshold: 1.0%, MAE: 0.03660496684129943, Sensitivity: 0.889795918367347, Specificity: 0.8888888888888888\n","threshold: 0.5%, MAE: 0.03660496684129943, Sensitivity: 0.9131274131274131, Specificity: 0.8518518518518519\n","threshold: 0.1%, MAE: 0.03660496684129943, Sensitivity: 0.8972125435540069, Specificity: 0.5925925925925926\n","\n","\n","Split no. 8\n","threshold: 5.0%, MAE: 0.039285998241129244, Sensitivity: 0.8741721854304636, Specificity: 1.0\n","threshold: 3.0%, MAE: 0.039285998241129244, Sensitivity: 0.8891752577319587, Specificity: 0.9629629629629629\n","threshold: 2.0%, MAE: 0.039285998241129244, Sensitivity: 0.8896396396396397, Specificity: 0.9629629629629629\n","threshold: 1.0%, MAE: 0.039285998241129244, Sensitivity: 0.892, Specificity: 0.9629629629629629\n","threshold: 0.5%, MAE: 0.039285998241129244, Sensitivity: 0.8901515151515151, Specificity: 0.8888888888888888\n","threshold: 0.1%, MAE: 0.039285998241129244, Sensitivity: 0.8647260273972602, Specificity: 0.8518518518518519\n","\n","\n","Split no. 9\n","threshold: 5.0%, MAE: 0.036815961734170004, Sensitivity: 0.8356643356643356, Specificity: 1.0\n","threshold: 3.0%, MAE: 0.036815961734170004, Sensitivity: 0.8722826086956522, Specificity: 1.0\n","threshold: 2.0%, MAE: 0.036815961734170004, Sensitivity: 0.8815165876777251, Specificity: 0.9629629629629629\n","threshold: 1.0%, MAE: 0.036815961734170004, Sensitivity: 0.9096638655462185, Specificity: 0.8888888888888888\n","threshold: 0.5%, MAE: 0.036815961734170004, Sensitivity: 0.9284294234592445, Specificity: 0.8888888888888888\n","threshold: 0.1%, MAE: 0.036815961734170004, Sensitivity: 0.895870736086176, Specificity: 0.7407407407407407\n","\n","\n"]}],"source":["for N in range(10):\n","  split_train = dic['train'][N] + dic['val'][N]\n","  split_test = dic['test'][N]\n","  train_indices, test_indices = [], []\n","  max_arr = []\n","\n","  for i in range(len(train_meta_info)):\n","    if train_meta_info[i][1] in split_train:\n","      train_indices.append(i)\n","  for i in range(len(test_meta_info)):   \n","    if test_meta_info[i][1] in split_test:\n","      test_indices.append(i)\n","  max_arr = np.max(train_samples[train_indices, :,:], axis=0)\n","\n","  train_data = LoadDataset(train_indices, True)\n","  test_data = LoadDataset(test_indices, False)\n","  train_loader = DataLoader(dataset=train_data, batch_size=32, shuffle=True)\n","  test_loader = DataLoader(dataset=test_data, batch_size=32)\n","\n","  # initializing model for split N\n","  model = VGG_convnet().to(device)\n","  criterion = nn.L1Loss()\n","  optimizer = optim.Adam(model.parameters(), lr=0.0001)\n","\n","  # train the model\n","  print(f'Split no. {N}')\n","  train(train_loader) ###\n","\n","  # model_path = '../../Experiments/Models/model' + str(N) + '.pt'\n","  # torch.save(model.state_dict(), model_path)\n","\n","  make_csv(N+1, max_arr)\n","\n","  for thr in thresholds:\n","      MAE, SN, SP = metric_calc(test_loader, thr)\n","      print(f'threshold: {thr*100}%, MAE: {MAE}, Sensitivity: {SN}, Specificity: {SP}')\n","      MAE_dic[thr].append(MAE)\n","      SN_dic[thr].append(SN)\n","      SP_dic[thr].append(SP)\n","  print()\n","  print()"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1666472138305,"user":{"displayName":"Rafeed Rahman","userId":"00328865681086178046"},"user_tz":-480},"id":"LGuKgp2WEvbV","outputId":"909f89de-eb10-4ba4-969b-ede8d9fa298e"},"outputs":[{"output_type":"stream","name":"stdout","text":["At threshold of 5.0%\n","MAE mean: 0.04, median: 0.04, std: 0.003\n","Sensitivity mean: 0.834, median: 0.836, std: 0.041\n","Specificity mean: 1.0, median: 1.0, std: 0.0\n","\n","At threshold of 3.0%\n","MAE mean: 0.04, median: 0.04, std: 0.003\n","Sensitivity mean: 0.847, median: 0.869, std: 0.04\n","Specificity mean: 0.989, median: 1.0, std: 0.017\n","\n","At threshold of 2.0%\n","MAE mean: 0.04, median: 0.04, std: 0.003\n","Sensitivity mean: 0.87, median: 0.875, std: 0.031\n","Specificity mean: 0.967, median: 0.963, std: 0.011\n","\n","At threshold of 1.0%\n","MAE mean: 0.04, median: 0.04, std: 0.003\n","Sensitivity mean: 0.897, median: 0.901, std: 0.029\n","Specificity mean: 0.889, median: 0.889, std: 0.083\n","\n","At threshold of 0.5%\n","MAE mean: 0.04, median: 0.04, std: 0.003\n","Sensitivity mean: 0.909, median: 0.919, std: 0.029\n","Specificity mean: 0.815, median: 0.852, std: 0.131\n","\n","At threshold of 0.1%\n","MAE mean: 0.04, median: 0.04, std: 0.003\n","Sensitivity mean: 0.9, median: 0.902, std: 0.038\n","Specificity mean: 0.637, median: 0.63, std: 0.155\n","\n"]}],"source":["for thr in thresholds:\n","    print(f'At threshold of {thr*100}%')\n","    mean_MAE = round(np.mean(MAE_dic[thr]), 3)\n","    med_MAE = round(np.median(MAE_dic[thr]), 3)\n","    std_MAE = round(np.std(MAE_dic[thr]), 3)\n","    print(f'MAE mean: {mean_MAE}, median: {med_MAE}, std: {std_MAE}')\n","        \n","    mean_SN = round(np.mean(SN_dic[thr]), 3)\n","    med_SN = round(np.median(SN_dic[thr]), 3)\n","    std_SN = round(np.std(SN_dic[thr]), 3)\n","    print(f'Sensitivity mean: {mean_SN}, median: {med_SN}, std: {std_SN}')\n","    \n","    mean_SP = round(np.mean(SP_dic[thr]), 3)\n","    med_SP = round(np.median(SP_dic[thr]), 3)\n","    std_SP = round(np.std(SP_dic[thr]), 3)\n","    print(f'Specificity mean: {mean_SP}, median: {med_SP}, std: {std_SP}')\n","    \n","    print()"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"provenance":[],"mount_file_id":"1XriPfqjGmtaANw5DFxlL7i-ux1QVECCs","authorship_tag":"ABX9TyNl6ZOJRqzXPzPyEIlLyh6f"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
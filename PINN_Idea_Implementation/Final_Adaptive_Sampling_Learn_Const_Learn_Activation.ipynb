{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"mount_file_id":"1wD7ePUdLq_MsFv7iw8lTTd-q6TNay_JM","authorship_tag":"ABX9TyMqqNSGdEi9DfN6gE4JJ9sO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","source":["import json\n","import pandas as pd\n","import os\n","import sys\n","import pickle\n","import numpy as np\n","import math\n","import random\n","import math\n","import torch\n","import torchvision \n","import torch.nn.functional as F  \n","import torchvision.datasets as datasets  \n","import torchvision.transforms as transforms  \n","from torch import optim  \n","from torch import nn  \n","from torch.utils.data import DataLoader  \n","from tqdm import tqdm  \n","from torch.utils.data import Dataset, DataLoader\n","from tqdm import tqdm"],"metadata":{"id":"KqdA3VzIpA-W","executionInfo":{"status":"ok","timestamp":1666715800667,"user_tz":-480,"elapsed":7,"user":{"displayName":"Rafeed Rahman","userId":"00328865681086178046"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["# parentPath = '/content/drive/MyDrive/PhD/Fragle_TSS/Tested_Algorithms/Large Bin Based Modeling/Dataset/Sig_10_Mb_10_Splits'\n","# os.chdir(parentPath)\n","\n","# partition_no = 15 # final\n","# uniform_epochs = 0\n","# non_uniform_epoch_no = 100 # final \n","# non_uniform_schedule_width = 5 # final"],"metadata":{"id":"9p2i1Iy92Wu1","executionInfo":{"status":"ok","timestamp":1666715800668,"user_tz":-480,"elapsed":7,"user":{"displayName":"Rafeed Rahman","userId":"00328865681086178046"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["parentPath = '/content/drive/MyDrive/PhD/Fragle_TSS/Tested_Algorithms/Large Bin Based Modeling/Dataset/Sig_10_Mb_10_Splits'\n","os.chdir(parentPath)\n","\n","partition_no = 15 \n","uniform_epochs = 70\n","non_uniform_epoch_no = 30  \n","non_uniform_schedule_width = 5"],"metadata":{"id":"z5t718RGMgds","executionInfo":{"status":"ok","timestamp":1666715800668,"user_tz":-480,"elapsed":6,"user":{"displayName":"Rafeed Rahman","userId":"00328865681086178046"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model, criterion, optimizer = None, None, None\n","train_indices, test_indices, train_partition_indices, train_y = [], [], [], []\n","train_samples, test_samples, max_arr = None, None, None\n","train_meta_info, test_meta_info = None, None\n","\n","json_file = open('../../meta_info_files/split_patient_wise.json')\n","dic = json.load(json_file)\n","myPath = os.getcwd()\n","\n","thresholds = [0.05, 0.03, 0.02, 0.01, 0.005, 0.001]\n","MAE_dic, SN_dic, SP_dic = {}, {}, {}\n","for thr in thresholds:\n","    MAE_dic[thr] = []\n","    SN_dic[thr] = []\n","    SP_dic[thr] = []"],"metadata":{"id":"HWcxDk0s2ZSE","executionInfo":{"status":"ok","timestamp":1666715800669,"user_tz":-480,"elapsed":6,"user":{"displayName":"Rafeed Rahman","userId":"00328865681086178046"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["loaded_dict = {}\n","with open('train_samples.pkl', 'rb') as f:\n","  loaded_dict = pickle.load(f)\n","train_meta_info = loaded_dict['meta'] \n","train_samples = loaded_dict['samples']\n","\n","train_y = []\n","for info in train_meta_info:\n","  train_y.append(info[-1])\n","train_y = np.array(train_y)\n","\n","loaded_dict = {}\n","with open('test_samples.pkl', 'rb') as f:\n","  loaded_dict = pickle.load(f)\n","test_meta_info = loaded_dict['meta'] \n","test_samples = loaded_dict['samples']\n","loaded_dict = {}"],"metadata":{"id":"os4radFn4rm5","executionInfo":{"status":"ok","timestamp":1666715804286,"user_tz":-480,"elapsed":3623,"user":{"displayName":"Rafeed Rahman","userId":"00328865681086178046"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["sums = np.sum(train_samples, axis=2)\n","train_samples = train_samples/ sums[:, :, np.newaxis]\n","\n","sums = np.sum(test_samples, axis=2)\n","test_samples = test_samples/ sums[:, :, np.newaxis]"],"metadata":{"id":"hDyQ6OWi6Mvt","executionInfo":{"status":"ok","timestamp":1666715805459,"user_tz":-480,"elapsed":1175,"user":{"displayName":"Rafeed Rahman","userId":"00328865681086178046"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["class LoadDataset(Dataset):\n","    def __init__(self, indices, train):\n","        self.indices = indices\n","        self.is_train = train\n","\n","    def __len__(self):\n","        return len(self.indices)\n","\n","    def __getitem__(self, index):\n","        ind = self.indices[index]\n","        if self.is_train == True:\n","          dataY = torch.tensor(train_meta_info[ind][-1])\n","          dataX = torch.tensor(train_samples[ind]/ max_arr)\n","        else:\n","          dataY = torch.tensor(test_meta_info[ind][-1])\n","          dataX = torch.tensor(test_samples[ind]/ max_arr)\n","        dataX = dataX.float()\n","        dataY = dataY.float()\n","        return (dataX, dataY)"],"metadata":{"id":"z97LcNUR2g6c","executionInfo":{"status":"ok","timestamp":1666715805461,"user_tz":-480,"elapsed":15,"user":{"displayName":"Rafeed Rahman","userId":"00328865681086178046"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# tmp = torch.tensor([5.0])\n","# print(len(tmp.size()))"],"metadata":{"id":"KruGLz2N98_S","executionInfo":{"status":"ok","timestamp":1666715805462,"user_tz":-480,"elapsed":16,"user":{"displayName":"Rafeed Rahman","userId":"00328865681086178046"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["class VGG_convnet(nn.Module):\n","\n","    def __init__(self, bin_no = 282, feature_no = 156):\n","\n","        super(VGG_convnet, self).__init__()\n","\n","        self.begin_linear1 = nn.Linear(feature_no, 64)\n","        self.begin_linear1_rw = torch.nn.Parameter(torch.randn(64))\n","        self.begin_linear2 = nn.Linear(64, 96)\n","        self.begin_linear2_rw = torch.nn.Parameter(torch.randn(96))\n","        self.begin_linear3 = nn.Linear(96, 128)\n","        self.begin_linear3_rw = torch.nn.Parameter(torch.randn(128))\n","        \n","        self.pos_emb1D = torch.nn.Parameter(torch.randn(bin_no, 128))\n","        self.const = torch.nn.Parameter(torch.tensor(0.01))\n","        \n","        # block 1:         \n","        self.conv1a = nn.Conv1d(in_channels=128, out_channels=32, kernel_size=3, padding=1)\n","        self.conv1b = nn.Conv1d(in_channels=32, out_channels=32, kernel_size=3, padding=1)\n","        self.LN1 = nn.LayerNorm(bin_no)\n","        \n","        # block 2:      \n","        self.conv2a = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n","        self.conv2b = nn.Conv1d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n","        self.LN2 = nn.LayerNorm(bin_no)\n","        \n","        # block 3:            \n","        self.conv3a =  nn.Conv1d(in_channels=64, out_channels=96, kernel_size=3, padding=1)\n","        self.conv3b =  nn.Conv1d(in_channels=96, out_channels=96, kernel_size=3, padding=1)\n","        self.LN3 = nn.LayerNorm(bin_no)\n","        \n","        #block 4:       \n","        self.conv4a = nn.Conv1d(in_channels=96, out_channels=128, kernel_size=3, padding=1)\n","        self.conv4b = nn.Conv1d(in_channels=128, out_channels=128, kernel_size=3, padding=1)\n","        self.LN4 = nn.LayerNorm(bin_no)\n","        \n","        # linear layers:\n","        self.end_linear1 = nn.Linear(128, 128)\n","        self.end_linear1_rw = torch.nn.Parameter(torch.randn(128))\n","        self.end_linear2 = nn.Linear(128, 64)\n","        self.end_linear2_rw = torch.nn.Parameter(torch.randn(64))\n","        self.end_linear3 = nn.Linear(64, 1)\n","\n","    def forward(self, x, targets, isTrain):\n","        x = self.begin_linear1(x)\n","        x = F.relu(x) + self.begin_linear1_rw * x * F.relu(x)\n","        x = self.begin_linear2(x)\n","        x = F.relu(x) + self.begin_linear2_rw * x * F.relu(x)\n","        x = self.begin_linear3(x)\n","        x = F.relu(x) + self.begin_linear3_rw * x * F.relu(x)\n","\n","        x = x + self.pos_emb1D\n","        x = x.permute(0, 2, 1) # 128, 282\n","        \n","        # block 1:  \n","        x = F.relu(self.conv1a(x)) # 32, 282\n","        x = F.relu(self.conv1b(x))\n","        x = self.LN1(x)\n","        \n","        # block 2:   \n","        x = F.relu(self.conv2a(x)) # 64, 282\n","        x = F.relu(self.conv2b(x))\n","        x = self.LN2(x)\n","\n","        # block 3:    \n","        x = F.relu(self.conv3a(x)) # 96, 282\n","        x = F.relu(self.conv3b(x))\n","        x = self.LN3(x)\n","        \n","        #block 4:      \n","        x = F.relu(self.conv4a(x)) # 128, 282\n","        x = F.relu(self.conv4b(x))\n","        x = self.LN4(x)\n","        \n","        y = torch.mean(x, axis=2) # 128\n","        \n","        # linear layers:   \n","        y = self.end_linear1(y)\n","        y = F.relu(y) + self.end_linear1_rw * y * F.relu(y)\n","        y = self.end_linear2(y)\n","        y = F.relu(y) + self.end_linear2_rw * y * F.relu(y)\n","        y = self.end_linear3(y)\n","        scores = y.squeeze()\n","\n","        final_loss = None\n","        if isTrain == True:\n","          numerator = self.const + torch.abs(scores - targets)\n","          denominator = self.const + targets\n","          loss = None\n","          if len(scores.size())==0:\n","            loss = torch.sum(numerator/denominator)\n","          else:\n","            loss = torch.sum(numerator/denominator)/ scores.size(0)\n","          final_loss = loss * loss\n","        \n","        return y, final_loss"],"metadata":{"id":"n0Hy1fCrAYbH","executionInfo":{"status":"ok","timestamp":1666715805463,"user_tz":-480,"elapsed":16,"user":{"displayName":"Rafeed Rahman","userId":"00328865681086178046"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["def metric_calc(loader, thr):\n","    running_MAE = 0\n","    num_batches = 0\n","    TP, TN, FP, FN = 0, 0, 0, 0\n","    model.eval()\n","    with torch.no_grad():\n","        for batch_idx, (data, targets) in enumerate(loader):\n","            # Get data to cuda if possible\n","            data = data.to(device=device)\n","            targets = targets.to(device=device)\n","\n","            # forward\n","            scores, _ = model(data, None, False)\n","            scores = scores.view(targets.size())\n","            \n","            # metric calc (MAE)\n","            #MAE_batch = L1criterion(scores, targets)\n","            MAE_batch = criterion(scores, targets)\n","            running_MAE = running_MAE + MAE_batch.item()\n","            \n","            # TP, TN, FP, FN\n","            for i in range(targets.size(0)):\n","                # if targets[i].item()>0.0 and targets[i].item()<0.01:\n","                if targets[i].item()>0.0 and targets[i].item()<thr:\n","                     continue\n","                elif scores[i].item()>thr and targets[i].item()>0.0:\n","                    TP += 1\n","                elif scores[i].item()<=thr and targets[i].item()>0.0:\n","                    FN+=1\n","                elif scores[i].item()>thr and targets[i].item()==0.0:\n","                    FP+=1\n","                elif scores[i].item()<=thr and targets[i].item()==0.0:\n","                    TN+=1\n","\n","            num_batches+=1\n","    \n","    MAE_final = running_MAE/num_batches\n","    SN_final = TP/(TP+FN)\n","    SP_final = TN/(TN+FP)\n","    \n","    return MAE_final, SN_final, SP_final"],"metadata":{"id":"jgcHv2hkEZAe","executionInfo":{"status":"ok","timestamp":1666715805463,"user_tz":-480,"elapsed":15,"user":{"displayName":"Rafeed Rahman","userId":"00328865681086178046"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["def make_csv(split_no, max_arr):\n","    csv_list = []\n","    test_y = []\n","    scores, sample_names = [], []\n","    \n","    model.eval()\n","    with torch.no_grad():\n","      for index in test_indices:\n","          sample_names.append(test_meta_info[index][1])\n","          dataX = np.copy(test_samples[index])\n","          dataX = dataX/ max_arr\n","          test_y.append(test_meta_info[index][-1])\n","\n","          dataX = torch.tensor(dataX)\n","          # dataX = dataX.permute(1, 0)     ########\n","          dataX = torch.unsqueeze(dataX, dim=0)\n","          dataX = dataX.float()\n","\n","          score, _ = model(dataX.to(device), None, False)\n","          scores.append(score.item())\n","        \n","    for i in range(len(scores)):\n","        tmp = []\n","        tmp.append(sample_names[i])\n","        tmp.append(scores[i])\n","        tmp.append(test_y[i])\n","        csv_list.append(tmp)\n","        \n","    folder = '/content/drive/MyDrive/PhD/Fragle_TSS/Tested_Algorithms/Large Bin Based Modeling/10_split_csv/'\n","    filePath = folder + 'test' + str(split_no) + '.csv'\n","    my_df = pd.DataFrame(csv_list)\n","    my_df.to_csv(filePath, index=False, header=['Sample_ID', 'Pred_Fraction', 'True_Fraction'])"],"metadata":{"id":"78h6fmNo39iT","executionInfo":{"status":"ok","timestamp":1666715805464,"user_tz":-480,"elapsed":15,"user":{"displayName":"Rafeed Rahman","userId":"00328865681086178046"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["def get_partition_probabilities(train_partition_indices):\n","  partition_losses = []\n","  model.eval()\n","  with torch.no_grad():\n","    for partition in train_partition_indices:\n","      dataX = train_samples[partition, :, :] \n","      dataX = dataX/ max_arr\n","      dataX = torch.tensor(dataX).float()\n","      dataY = []\n","      for i in range(len(partition)):\n","        ind = partition[i]\n","        dataY.append(train_y[ind])\n","      dataY = torch.tensor(dataY).float()\n","\n","      _, loss = model(dataX.to(device), dataY.to(device), True)\n","      partition_losses.append(loss.item())\n","\n","  parititon_probabilities = partition_losses/ np.sum(partition_losses)\n","  return parititon_probabilities"],"metadata":{"id":"ro6ga2Zfldyg","executionInfo":{"status":"ok","timestamp":1666715805464,"user_tz":-480,"elapsed":14,"user":{"displayName":"Rafeed Rahman","userId":"00328865681086178046"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# total 140 samples\n","# 3 partitions -> 50, 50, 40\n","# Equal probability -> 1/3 = 0.3333\n","# 3 probabilities -> 0.5, 0.4, 0.1\n","# sample no from partition 1: 50 X (0.5/ 0.3333), partition 2: 50 X (0.4/ 0.3333) and partition 3: 40 X (0.1/ 0.3333)"],"metadata":{"id":"nR1Xwhgj-16q","executionInfo":{"status":"ok","timestamp":1666715805465,"user_tz":-480,"elapsed":15,"user":{"displayName":"Rafeed Rahman","userId":"00328865681086178046"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["def sample_train_indices(train_partition_indices, partition_probabilities):\n","  chosen_indices = []\n","  uniform_probability = float(1/ len(train_partition_indices))\n","  for i in range(len(train_partition_indices)):\n","    sample_no = round(len(train_partition_indices[i]) * (partition_probabilities[i]/ uniform_probability))\n","    chosen_indices.extend( random.choices(train_partition_indices[i], k=sample_no) )\n","  return chosen_indices"],"metadata":{"id":"VzQmhd7KFttS","executionInfo":{"status":"ok","timestamp":1666715805465,"user_tz":-480,"elapsed":14,"user":{"displayName":"Rafeed Rahman","userId":"00328865681086178046"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["def get_train_indices(uniform_sample_ratio, train_partition_indices):\n","  non_uniform_sample_ratio = 1 - uniform_sample_ratio\n","  train_indices = []\n","  uniform_prob = float(1/ len(train_partition_indices)) * uniform_sample_ratio\n","  uniform_probabilities = []\n","  for i in range(len(train_partition_indices)):\n","    uniform_probabilities.append(uniform_prob)\n","  non_uniform_probabilities = get_partition_probabilities(train_partition_indices)\n","  for i in range(len(non_uniform_probabilities)):\n","    non_uniform_probabilities[i] = non_uniform_probabilities[i] * non_uniform_sample_ratio\n","  \n","  train_indices.extend(sample_train_indices(train_partition_indices, uniform_probabilities))\n","  train_indices.extend(sample_train_indices(train_partition_indices, non_uniform_probabilities))\n","  return train_indices"],"metadata":{"id":"H2RSxSAvsYOb","executionInfo":{"status":"ok","timestamp":1666715805466,"user_tz":-480,"elapsed":14,"user":{"displayName":"Rafeed Rahman","userId":"00328865681086178046"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["def train(train_indices, train_partition_indices, test_loader):\n","  train_data = LoadDataset(train_indices, True)\n","  train_loader = DataLoader(dataset=train_data, batch_size=32, shuffle=True)\n","  for epoch in range(uniform_epochs):\n","    for batch_idx, (data, targets) in enumerate(train_loader):\n","        data = data.to(device=device)\n","        data.requires_grad_()\n","        targets = targets.to(device=device)\n","        scores, loss = model(data, targets, True)\n","        scores = scores.view(targets.size())\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","    # if epoch % 5 == 0:\n","    #   print(f'uniform epoch {epoch}')\n","    #   for thr in thresholds:\n","    #     MAE, SN, SP = metric_calc(test_loader, thr)\n","    #     print(f'threshold: {thr*100}%, MAE: {MAE}, Sensitivity: {SN}, Specificity: {SP}')\n","\n","  T = non_uniform_schedule_width\n","  model.const.requires_grad = False\n","  # print(model.const.item())\n","  for epoch in range(non_uniform_epoch_no):\n","    Tc = epoch % T\n","    uniform_sample_ratio = 0.5 * (1 + math.cos(math.pi * (Tc/ T))) # turn this variable to 0 to stop uniform sampling\n","    # uniform_sample_ratio = 0\n","    train_indices = get_train_indices(uniform_sample_ratio, train_partition_indices)\n","    train_data = LoadDataset(train_indices, True)\n","    train_loader = DataLoader(dataset=train_data, batch_size=32, shuffle=True)\n","    for batch_idx, (data, targets) in enumerate(train_loader):\n","        data = data.to(device=device)\n","        data.requires_grad_()\n","        targets = targets.to(device=device)\n","        scores, loss = model(data, targets, True)\n","        scores = scores.view(targets.size())\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","    # if (epoch+1) % 5 == 0:\n","    #   print(f'non-uniform epoch {epoch}')\n","    #   print(model.const.item())\n","    #   for thr in thresholds:\n","    #     MAE, SN, SP = metric_calc(test_loader, thr)\n","    #     print(f'threshold: {thr*100}%, MAE: {MAE}, Sensitivity: {SN}, Specificity: {SP}')"],"metadata":{"id":"7VwVutDjuLW4","executionInfo":{"status":"ok","timestamp":1666715805466,"user_tz":-480,"elapsed":14,"user":{"displayName":"Rafeed Rahman","userId":"00328865681086178046"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["for N in range(10):\n","  split_train = dic['train'][N] + dic['val'][N]\n","  split_test = dic['test'][N]\n","  train_indices, test_indices, train_indices_output = [], [], []\n","  max_arr = []\n","\n","  for i in range(len(train_meta_info)):\n","    if train_meta_info[i][1] in split_train:\n","      train_indices.append(i)\n","      train_indices_output.append((i, train_meta_info[i][-1]))\n","  train_indices_output = sorted(train_indices_output, key = lambda x : x[1])\n","  train_indices_sorted = []\n","  for item in train_indices_output:\n","    train_indices_sorted.append(item[0])\n","  partition_step = math.ceil(len(train_indices_output)/ partition_no)\n","  train_partitions, train_partition_indices = [], []\n","  for i in range(0, len(train_indices_output), partition_step):\n","    train_partitions.append([i, min(i+partition_step, len(train_indices_output))])\n","  for partition in train_partitions:\n","    train_partition_indices.append(train_indices_sorted[partition[0]: partition[1]])\n","\n","  for i in range(len(test_meta_info)):   \n","    if test_meta_info[i][1] in split_test:\n","      test_indices.append(i)\n","  max_arr = np.max(train_samples[train_indices, :,:], axis=0)\n","\n","  test_data = LoadDataset(test_indices, False)\n","  test_loader = DataLoader(dataset=test_data, batch_size=32)\n","\n","  # initializing model for split N\n","  model = VGG_convnet().to(device)\n","  criterion = nn.L1Loss()\n","  optimizer = optim.Adam(model.parameters(), lr=0.0001)\n","\n","  # train the model\n","  print(f'Split no. {N}')\n","  train(train_indices, train_partition_indices, test_loader) \n","\n","  # # model_path = '../../Experiments/Models/model' + str(N) + '.pt'\n","  # # torch.save(model.state_dict(), model_path)\n","  make_csv(N+1, max_arr)\n","\n","  for thr in thresholds:\n","      MAE, SN, SP = metric_calc(test_loader, thr)\n","      print(f'threshold: {thr*100}%, MAE: {MAE}, Sensitivity: {SN}, Specificity: {SP}')\n","      MAE_dic[thr].append(MAE)\n","      SN_dic[thr].append(SN)\n","      SP_dic[thr].append(SP)\n","  print()\n","  print()\n","  "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9uqjfrrZ4n_D","outputId":"4a56fe73-8ebc-4908-b04f-2615c9f7158f","executionInfo":{"status":"ok","timestamp":1666717110040,"user_tz":-480,"elapsed":1304587,"user":{"displayName":"Rafeed Rahman","userId":"00328865681086178046"}}},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Split no. 0\n","threshold: 5.0%, MAE: 0.03749666536565532, Sensitivity: 0.8590604026845637, Specificity: 1.0\n","threshold: 3.0%, MAE: 0.03749666536565532, Sensitivity: 0.8605263157894737, Specificity: 1.0\n","threshold: 2.0%, MAE: 0.03749666536565532, Sensitivity: 0.8525345622119815, Specificity: 0.9629629629629629\n","threshold: 1.0%, MAE: 0.03749666536565532, Sensitivity: 0.860655737704918, Specificity: 0.9629629629629629\n","threshold: 0.5%, MAE: 0.03749666536565532, Sensitivity: 0.8679611650485437, Specificity: 0.9259259259259259\n","threshold: 0.1%, MAE: 0.03749666536565532, Sensitivity: 0.8347978910369068, Specificity: 0.8148148148148148\n","\n","\n","Split no. 1\n","threshold: 5.0%, MAE: 0.041385510597716675, Sensitivity: 0.7748344370860927, Specificity: 1.0\n","threshold: 3.0%, MAE: 0.041385510597716675, Sensitivity: 0.7881136950904393, Specificity: 1.0\n","threshold: 2.0%, MAE: 0.041385510597716675, Sensitivity: 0.8058690744920993, Specificity: 0.9629629629629629\n","threshold: 1.0%, MAE: 0.041385510597716675, Sensitivity: 0.8256513026052105, Specificity: 0.9629629629629629\n","threshold: 0.5%, MAE: 0.041385510597716675, Sensitivity: 0.8387096774193549, Specificity: 0.9259259259259259\n","threshold: 0.1%, MAE: 0.041385510597716675, Sensitivity: 0.8198970840480274, Specificity: 0.8518518518518519\n","\n","\n","Split no. 2\n","threshold: 5.0%, MAE: 0.039407014580709596, Sensitivity: 0.8185053380782918, Specificity: 1.0\n","threshold: 3.0%, MAE: 0.039407014580709596, Sensitivity: 0.8209366391184573, Specificity: 1.0\n","threshold: 2.0%, MAE: 0.039407014580709596, Sensitivity: 0.8729016786570744, Specificity: 0.9629629629629629\n","threshold: 1.0%, MAE: 0.039407014580709596, Sensitivity: 0.9002123142250531, Specificity: 0.9629629629629629\n","threshold: 0.5%, MAE: 0.039407014580709596, Sensitivity: 0.8995983935742972, Specificity: 0.7777777777777778\n","threshold: 0.1%, MAE: 0.039407014580709596, Sensitivity: 0.8913043478260869, Specificity: 0.6296296296296297\n","\n","\n","Split no. 3\n","threshold: 5.0%, MAE: 0.04120065094056455, Sensitivity: 0.8494983277591973, Specificity: 1.0\n","threshold: 3.0%, MAE: 0.04120065094056455, Sensitivity: 0.8626943005181347, Specificity: 0.9629629629629629\n","threshold: 2.0%, MAE: 0.04120065094056455, Sensitivity: 0.8642533936651584, Specificity: 0.9629629629629629\n","threshold: 1.0%, MAE: 0.04120065094056455, Sensitivity: 0.9076305220883534, Specificity: 0.8888888888888888\n","threshold: 0.5%, MAE: 0.04120065094056455, Sensitivity: 0.9163498098859315, Specificity: 0.8518518518518519\n","threshold: 0.1%, MAE: 0.04120065094056455, Sensitivity: 0.8969072164948454, Specificity: 0.6296296296296297\n","\n","\n","Split no. 4\n","threshold: 5.0%, MAE: 0.0367883792231706, Sensitivity: 0.8538205980066446, Specificity: 1.0\n","threshold: 3.0%, MAE: 0.0367883792231706, Sensitivity: 0.8730569948186528, Specificity: 1.0\n","threshold: 2.0%, MAE: 0.0367883792231706, Sensitivity: 0.8823529411764706, Specificity: 0.9629629629629629\n","threshold: 1.0%, MAE: 0.0367883792231706, Sensitivity: 0.9056224899598394, Specificity: 0.9629629629629629\n","threshold: 0.5%, MAE: 0.0367883792231706, Sensitivity: 0.9144486692015209, Specificity: 0.8148148148148148\n","threshold: 0.1%, MAE: 0.0367883792231706, Sensitivity: 0.8848797250859106, Specificity: 0.6666666666666666\n","\n","\n","Split no. 5\n","threshold: 5.0%, MAE: 0.03381715638732368, Sensitivity: 0.7759197324414716, Specificity: 1.0\n","threshold: 3.0%, MAE: 0.03381715638732368, Sensitivity: 0.7395833333333334, Specificity: 1.0\n","threshold: 2.0%, MAE: 0.03381715638732368, Sensitivity: 0.7431818181818182, Specificity: 1.0\n","threshold: 1.0%, MAE: 0.03381715638732368, Sensitivity: 0.7439516129032258, Specificity: 1.0\n","threshold: 0.5%, MAE: 0.03381715638732368, Sensitivity: 0.7729007633587787, Specificity: 1.0\n","threshold: 0.1%, MAE: 0.03381715638732368, Sensitivity: 0.7655172413793103, Specificity: 0.9629629629629629\n","\n","\n","Split no. 6\n","threshold: 5.0%, MAE: 0.050675091545351526, Sensitivity: 0.7010309278350515, Specificity: 1.0\n","threshold: 3.0%, MAE: 0.050675091545351526, Sensitivity: 0.6870026525198939, Specificity: 1.0\n","threshold: 2.0%, MAE: 0.050675091545351526, Sensitivity: 0.6836027713625866, Specificity: 1.0\n","threshold: 1.0%, MAE: 0.050675091545351526, Sensitivity: 0.6912065439672802, Specificity: 1.0\n","threshold: 0.5%, MAE: 0.050675091545351526, Sensitivity: 0.6963249516441006, Specificity: 1.0\n","threshold: 0.1%, MAE: 0.050675091545351526, Sensitivity: 0.6649214659685864, Specificity: 0.9629629629629629\n","\n","\n","Split no. 7\n","threshold: 5.0%, MAE: 0.039452649568292225, Sensitivity: 0.8361774744027304, Specificity: 1.0\n","threshold: 3.0%, MAE: 0.039452649568292225, Sensitivity: 0.8174603174603174, Specificity: 0.9629629629629629\n","threshold: 2.0%, MAE: 0.039452649568292225, Sensitivity: 0.7972350230414746, Specificity: 0.9629629629629629\n","threshold: 1.0%, MAE: 0.039452649568292225, Sensitivity: 0.7959183673469388, Specificity: 0.9629629629629629\n","threshold: 0.5%, MAE: 0.039452649568292225, Sensitivity: 0.806949806949807, Specificity: 0.9259259259259259\n","threshold: 0.1%, MAE: 0.039452649568292225, Sensitivity: 0.7909407665505227, Specificity: 0.8518518518518519\n","\n","\n","Split no. 8\n","threshold: 5.0%, MAE: 0.03500817419791764, Sensitivity: 0.8774834437086093, Specificity: 1.0\n","threshold: 3.0%, MAE: 0.03500817419791764, Sensitivity: 0.884020618556701, Specificity: 1.0\n","threshold: 2.0%, MAE: 0.03500817419791764, Sensitivity: 0.8918918918918919, Specificity: 0.9629629629629629\n","threshold: 1.0%, MAE: 0.03500817419791764, Sensitivity: 0.914, Specificity: 0.8518518518518519\n","threshold: 0.5%, MAE: 0.03500817419791764, Sensitivity: 0.9147727272727273, Specificity: 0.8518518518518519\n","threshold: 0.1%, MAE: 0.03500817419791764, Sensitivity: 0.8921232876712328, Specificity: 0.8148148148148148\n","\n","\n","Split no. 9\n","threshold: 5.0%, MAE: 0.03492312213139875, Sensitivity: 0.916083916083916, Specificity: 1.0\n","threshold: 3.0%, MAE: 0.03492312213139875, Sensitivity: 0.907608695652174, Specificity: 0.9629629629629629\n","threshold: 2.0%, MAE: 0.03492312213139875, Sensitivity: 0.9075829383886256, Specificity: 0.9629629629629629\n","threshold: 1.0%, MAE: 0.03492312213139875, Sensitivity: 0.8907563025210085, Specificity: 0.9259259259259259\n","threshold: 0.5%, MAE: 0.03492312213139875, Sensitivity: 0.8906560636182903, Specificity: 0.9259259259259259\n","threshold: 0.1%, MAE: 0.03492312213139875, Sensitivity: 0.8491921005385996, Specificity: 0.7407407407407407\n","\n","\n"]}]},{"cell_type":"code","source":["for thr in thresholds:\n","    print(f'At threshold of {thr*100}%')\n","    mean_MAE = round(np.mean(MAE_dic[thr]), 3)\n","    med_MAE = round(np.median(MAE_dic[thr]), 3)\n","    std_MAE = round(np.std(MAE_dic[thr]), 3)\n","    print(f'MAE mean: {mean_MAE}, median: {med_MAE}, std: {std_MAE}')\n","        \n","    mean_SN = round(np.mean(SN_dic[thr]), 3)\n","    med_SN = round(np.median(SN_dic[thr]), 3)\n","    std_SN = round(np.std(SN_dic[thr]), 3)\n","    print(f'Sensitivity mean: {mean_SN}, median: {med_SN}, std: {std_SN}')\n","    \n","    mean_SP = round(np.mean(SP_dic[thr]), 3)\n","    med_SP = round(np.median(SP_dic[thr]), 3)\n","    std_SP = round(np.std(SP_dic[thr]), 3)\n","    print(f'Specificity mean: {mean_SP}, median: {med_SP}, std: {std_SP}')\n","    \n","    print()"],"metadata":{"id":"LGuKgp2WEvbV","executionInfo":{"status":"ok","timestamp":1666717110040,"user_tz":-480,"elapsed":17,"user":{"displayName":"Rafeed Rahman","userId":"00328865681086178046"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"c7a7f808-ffce-44d5-acd3-f714f91ff4b9"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["At threshold of 5.0%\n","MAE mean: 0.039, median: 0.038, std: 0.005\n","Sensitivity mean: 0.826, median: 0.843, std: 0.058\n","Specificity mean: 1.0, median: 1.0, std: 0.0\n","\n","At threshold of 3.0%\n","MAE mean: 0.039, median: 0.038, std: 0.005\n","Sensitivity mean: 0.824, median: 0.841, std: 0.066\n","Specificity mean: 0.989, median: 1.0, std: 0.017\n","\n","At threshold of 2.0%\n","MAE mean: 0.039, median: 0.038, std: 0.005\n","Sensitivity mean: 0.83, median: 0.858, std: 0.068\n","Specificity mean: 0.97, median: 0.963, std: 0.015\n","\n","At threshold of 1.0%\n","MAE mean: 0.039, median: 0.038, std: 0.005\n","Sensitivity mean: 0.844, median: 0.876, std: 0.074\n","Specificity mean: 0.948, median: 0.963, std: 0.044\n","\n","At threshold of 0.5%\n","MAE mean: 0.039, median: 0.038, std: 0.005\n","Sensitivity mean: 0.852, median: 0.879, std: 0.07\n","Specificity mean: 0.9, median: 0.926, std: 0.07\n","\n","At threshold of 0.1%\n","MAE mean: 0.039, median: 0.038, std: 0.005\n","Sensitivity mean: 0.829, median: 0.842, std: 0.07\n","Specificity mean: 0.793, median: 0.815, std: 0.117\n","\n"]}]}]}